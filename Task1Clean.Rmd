---
title: "Task 1 - Cleaning"
author: "Suharkov MP"
date: "14 12 2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries, warning=F, message=F}
library(dplyr)
library(ggplot2)
library(stringr)
```

## Overview
In my work I will use the en_US locale of the dataset. To decide what part of this huge pack of data to use, I want to look at the distribution of lengths for each of three files. According to this, I will load while files onto memory and filter data afterwards. Next step is to look at these sets' percentiles to decide, how much data will be used for building model.
```{r overview1, warning=F, message=F}
blogs <- as.data.frame(readLines('./final/en_US/en_US.blogs.txt'))
names(blogs) <- c('Text')
blogs <- blogs %>% mutate(length = nchar(blogs$Text))
quantile(blogs$length, c(.01, .05, .1, .2, .3, .4, .5, .6, .7, .8, .9, .95, .99))
```

```{r overview2, warning=F, message=F}
news <- as.data.frame(readLines('./final/en_US/en_US.news.txt'))
names(news) <- c('Text')
news <- news %>% mutate(length = nchar(news$Text))
quantile(news$length, c(.01, .05, .1, .2, .3, .4, .5, .6, .7, .8, .9, .95, .99))
```

```{r overview3, warning=F, message=F}
twitter <- as.data.frame(readLines('./final/en_US/en_US.twitter.txt'))
names(twitter) <- c('Text')
twitter <- twitter %>% mutate(length = nchar(twitter$Text))
quantile(twitter$length, c(.01, .05, .1, .2, .3, .4, .5, .6, .7, .8, .9, .95, .99))
```
Well, I see that the most informative from all these files is the one containing blogs. Twitter's 99%-percentile is less than others' 30-50%. Maybe I will not use twitter for teaching my model at all? But it contains text data posted by common users, not by newsmakers or bloggers. And they could be the most common users of my model, so some of the text should be left, but how much? Let it be the 90%-percentile. It will contain about 236k lines of text, that is about a quarter of blogs' lines. So, in other two sets I will leave the data with similar length (124 symbols, let's round them up to **128**)
```{r filtering, warning=F, message=F}
blogs <- blogs %>% filter(length > 127)
news <- news %>% filter(length > 127)
twitter <- twitter %>% filter(length > 127)
```
Next, I will write these three to a new dataframe, then continue working with it.
```{r write, warning=F, message=F}
sample <- bind_rows(blogs, news, twitter)
```

## Tokenization
Several steps will be taken here.

First I want to do is to transform all data to lowercase:
```{r token, warning=F, message=F}
sample$Text <- tolower(sample$Text)
```

Next, remove all except letters, numbers and punctuation and save result:
```{r clean, warning=F, message=F}
sample$Text <- str_replace_all(sample$Text, '[^a-zA-Z0-9 .,:;-]', '')
write.csv(sample, 'sample.csv')
```
