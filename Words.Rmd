---
title: "Words - Capstone"
author: "Suharkov MP"
date: "14 12 2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries, warning=F, message=F}
library(dplyr) #dataframe formatting
library(stringr) #string formatting
library(readr) #txt to string
library(tm) #data analisys
```

# Overview

In my work I will use the en_US locale of the dataset. To decide what part of this huge pack of data to use, I want to look at the distribution of lengths for each of three files. According to this, I will load while files onto memory and filter data afterwards. Next step is to look at these sets' percentiles to decide, how much data will be used for building model.
```{r overview1, warning=F, message=F}
blogs <- as.data.frame(readLines('./final/en_US/en_US.blogs.txt'))
names(blogs) <- c('Text')
blogs <- blogs %>% mutate(length = nchar(blogs$Text))
quantile(blogs$length, c(.01, .05, .1, .2, .3, .4, .5, .6, .7, .8, .9, .95, .99))
```

```{r overview2, warning=F, message=F}
news <- as.data.frame(readLines('./final/en_US/en_US.news.txt'))
names(news) <- c('Text')
news <- news %>% mutate(length = nchar(news$Text))
quantile(news$length, c(.01, .05, .1, .2, .3, .4, .5, .6, .7, .8, .9, .95, .99))
```

```{r overview3, warning=F, message=F}
twitter <- as.data.frame(readLines('./final/en_US/en_US.twitter.txt'))
names(twitter) <- c('Text')
twitter <- twitter %>% mutate(length = nchar(twitter$Text))
quantile(twitter$length, c(.01, .05, .1, .2, .3, .4, .5, .6, .7, .8, .9, .95, .99))
```

# Task 1 - Getting and cleaning the data

Well, I see that the most informative from all these files is the one containing blogs. Twitter's 99%-percentile is less than others' 30-50%. I will not use twitter for teaching my model at all. How much of blogs and news should be left? Let it be the 99%-percentiles of blogs and news (538 and 363 symbols respectively). It will contain about 10k lines of text:
```{r filtering, warning=F, message=F}
blogs <- blogs %>% filter(length > 1120)
news <- news %>% filter(length > 631)
```

Next, I will write these three to a new dataframe, then continue working with it.
```{r write, warning=F, message=F}
sample <- bind_rows(blogs, news)
```

## Tokenization
Several steps will be taken here.

Finally I've found a **tm** package to do all the backend for me, so the rest of this file will be changed.

## Profanity filtering

```{r prof, warning=F, message=F}

```

Now I've removed all unnecessary data, only tokenized text left. Questions' time.

# Task 2 - Exploratory Data Analysis

## Question 1
Some words are more frequent than others - what are the distributions of word frequencies? 
```{r q1, warning=F, message=F}

```


## Question 2
What are the frequencies of 2-grams and 3-grams in the dataset? 
```{r q2, warning=F, message=F}

```


## Question 3
How many unique words do you need in a frequency sorted dictionary to cover 50% of all word instances in the language? 90%? 
```{r q3, warning=F, message=F}

```


## Question 4
How do you evaluate how many of the words come from foreign languages? 
```{r q4, warning=F, message=F}

```


## Question 5
Can you think of a way to increase the coverage -- identifying words that may not be in the corpora or using a smaller number of words in the dictionary to cover the same number of phrases?
```{r q5, warning=F, message=F}

```

# Task 3 - Modeling

